# -*- coding: utf-8 -*-
"""DeepWalk-HPP-California.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nt15FlOzYxvrmLxToI_VnXS7OH_6pQ4l

# Deepwalk-HPP

Deepwalk-HPP is a Python repository that implements house representation learning using DeepWalk, a network representation learning technique.
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import networkx as nx
from scipy.spatial.distance import pdist, squareform
from tqdm import tqdm
from pyproj import Transformer, CRS

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_percentage_error , r2_score, mean_squared_error

from gensim.models import Word2Vec
import json
import plotly.express as px
import networkx as nx
from geopy.distance import geodesic
from scipy.spatial import cKDTree
import random

# Set the seed
seed = 42
random.seed(seed)
np.random.seed(seed)

# !pip install nbformat

"""# Load the real estate dataset"""

data= pd.read_csv('data/California-housing.csv')
data.head()

data.columns

# تبدیل ستون 'ocean_proximity' به One-Hot Encoding
data = pd.get_dummies(data, columns=['ocean_proximity'], drop_first=True)
data.columns

data.shape

# Check if there are any NaN values in the DataFrame
has_nan = data.isnull().any().any()
print("Does the DataFrame contain NaN values?", has_nan)

# Remove rows with NaN values
data_cleaned = data.dropna()

# Check if there are any NaN values after cleaning
has_nan_after_cleaning = data_cleaned.isnull().any().any()
print("Does the cleaned DataFrame contain NaN values?", has_nan_after_cleaning)

# Display the first few rows of the cleaned DataFrame
# print(data_cleaned.head())
print(data_cleaned.shape)

data = data_cleaned

filtered_data = data.copy()

"""## Filter the data to focus on properties within a high demand region"""

# filtered_data = data[(data['longitude'] >= -124) & (data['longitude'] <= -120) &
#                  (data['latitude'] >= 36) & (data['latitude'] <= 40)]

filtered_data.shape

"""In another dataset a subset of data points are selected, which is not needed here, but the cell is preserved for more similarity of two codes."""

np.random.seed(42)
num_selected = filtered_data.shape[0] # 7000 in another dataset, all data points, here
shuffle_indices = np.random.choice(np.arange(filtered_data.shape[0]), size=num_selected, replace=False,)
df = filtered_data.iloc[shuffle_indices].reset_index(drop=True)
df['id'] = df.index  # Add this line to create a unique identifier for each house
df.shape, df.head(2)

# df = data_cleaned.sample(5000, random_state=42)
# df['id'] = df.index
# df.shape, df.head(2)

"""Visualize the spatial distribution of the selected properties using a scatter plot"""

fig = px.scatter(
    df,
    x="longitude",
    y="latitude",
    color="price",
    color_continuous_scale="turbo",
    width=400,
    height=400
)
fig.update_traces(marker=dict(size=4, opacity=0.9))
fig.show()

"""Creates a NetworkX graph of houses in the dataset"""

import networkx as nx
import pandas as pd
from sklearn.preprocessing import StandardScaler
from scipy.spatial import cKDTree
import numpy as np
from tqdm import tqdm

def create_graph_from_dataframe(df, distance_threshold):
    """
    Create a graph from a DataFrame of property features, with edges based on
    geographic proximity and weighted by property attributes.

    Parameters:
    - df: pd.DataFrame containing property features.
    - distance_threshold: float, the distance threshold in meters for creating edges.

    Returns:
    - G: networkx.Graph, the generated graph with nodes and weighted edges.
    """
    # Separate numerical and binary features
    # numeric_features = ['area_sq_m', 'age_years', 'floor_number', 'number_of_bedrooms']
    # binary_features = ['elevator', 'parking', 'storage', 'balcony', 'parquet',
    #                   'ceramic_flooring', 'stone_façade', 'garden', 'renovated']
    numeric_features = ['median_income', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households']
    # binary_features = ['ocean_proximity']
    # Normalize numeric features
    scaler = StandardScaler()
    df[numeric_features] = scaler.fit_transform(df[numeric_features])

    # Build spatial index with cKDTree
    coordinates = df[['latitude', 'longitude']].to_numpy()
    tree = cKDTree(coordinates)

    # Initialize graph
    G = nx.Graph()

    # Add nodes with attributes using the new 'id' column
    for i, row in df.iterrows():
        G.add_node(row['id'], **row.to_dict())  # Use the 'id' as the node identifier

    # Find pairs of nodes within the threshold distance
    pairs = tree.query_pairs(distance_threshold / 111000)  # Approximate meter-to-degree conversion

    # Define a function to compute edge weight
    def calculate_weight(df, idx1, idx2):
        geo_distance = np.linalg.norm(coordinates[idx1] - coordinates[idx2])


        weight = (1 / (1 + geo_distance))
        return weight

    # Add edges with weights
    for idx1, idx2 in tqdm(pairs):
        weight = calculate_weight(df, idx1, idx2)
        if weight != 0:
            G.add_edge(df.at[idx1, 'id'], df.at[idx2, 'id'], weight=weight)  # Use ids for edges

    return G

# Commented out IPython magic to ensure Python compatibility.
# %%time
# G = create_graph_from_dataframe(df, distance_threshold=4000)

def is_complete_graph(G):
    n = G.number_of_nodes()
    if n < 2:
        return True  # A graph with 0 or 1 node is considered complete
    max_edges = n * (n - 1) // 2
    print(G.number_of_edges(), max_edges)
    return G.number_of_edges() == max_edges

print("G is complete:", is_complete_graph(G))



df.head(1)

import random
import plotly.graph_objects as go
import networkx as nx
import pandas as pd

# Assuming df is your dataframe with columns 'id', 'longitude', and 'latitude',
# and G is the network graph created from the dataframe `df`.

# Step 1: Select a random node from the graph
random_node = random.choice(list(G.nodes))

# Step 2: Get edges connected to the random node
edges = G.edges(random_node)

# Step 3: Collect all nodes to plot: random node plus its neighbors
nodes_to_plot = [random_node] + list(G.neighbors(random_node))

# Step 4: Filter the dataframe to get coordinates for selected nodes
selected_houses = df[df['id'].isin(nodes_to_plot)]

# Step 5: Create a Geo Scatter Plot for visible nodes
fig = go.Figure()

# Step 6: Plot edges
for u, v in edges:
    if u in selected_houses['id'].values and v in selected_houses['id'].values:
        u_coords = selected_houses[selected_houses['id'] == u][['longitude', 'latitude']].values[0]
        v_coords = selected_houses[selected_houses['id'] == v][['longitude', 'latitude']].values[0]

        # Draw edges on the plot
        fig.add_trace(go.Scattergeo(
            lon=[u_coords[0], v_coords[0]],
            lat=[u_coords[1], v_coords[1]],
            mode='lines',
            line=dict(width=1, color='blue'),
            showlegend=False
        ))

# Step 7: Highlight the random node in red
random_node_coords = selected_houses[selected_houses['id'] == random_node]
fig.add_trace(go.Scattergeo(
    lon=random_node_coords['longitude'],
    lat=random_node_coords['latitude'],
    mode='markers',
    marker=dict(size=10, color='red', symbol='circle'),  # Random node in red
    name='Random Node'
))

# Step 8: Adding selected (neighbors) houses to the plot
fig.add_trace(go.Scattergeo(
    lon=selected_houses['longitude'],
    lat=selected_houses['latitude'],
    mode='markers',
    marker=dict(size=6, color='green', symbol='square'),  # Selected neighbors in green
    name='Selected Neighbors'
))

# Step 9: Adding a subset of other houses to the plot (not selected)
all_houses = df[df['id'].isin(list(G.nodes))]
other_houses = all_houses[~all_houses['id'].isin(nodes_to_plot)]  # Exclude selected nodes

# Randomly select a subset of other houses (e.g., 100)
subset_size = min(10000, len(other_houses))  # Ensure we don't exceed the available number
other_houses_sampled = other_houses.sample(n=subset_size, random_state=1)  # Sample random houses

fig.add_trace(go.Scattergeo(
    lon=other_houses_sampled['longitude'],
    lat=other_houses_sampled['latitude'],
    mode='markers',
    marker=dict(size=5, color='blue', symbol='circle'),  # Other houses in blue
    name='Other Houses'
))

# Step 10: Update layout to focus on a high demand area
fig.update_layout(
    title='Houses with Edges from a Random Node',
    geo=dict(
        scope='asia',  # Set the scope to Asia for better overview
        projection_type='mercator',
        showland=True,
        landcolor='lightgray',
        countrycolor='white',
        fitbounds="locations",  # Automatically fits the map to all locations
        center=dict(lon=random_node_coords['longitude'].values[0], lat=random_node_coords['latitude'].values[0])  # Center on random node
    )
)

# Step 11: Show the final plot
fig.show()

import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt

# Assuming G is your NetworkX graph created from the df DataFrame
# and df contains the relevant house data with longitude and latitude

# Step 1: Calculate basic statistics
num_nodes = G.number_of_nodes()
num_edges = G.number_of_edges()
degrees = [len(list(G.neighbors(node))) for node in G.nodes]  # List of number of neighbors for each node

# Step 2: Compute minimum, maximum, and average number of neighbors
min_neighbors = min(degrees)
max_neighbors = max(degrees)
avg_neighbors = sum(degrees) / num_nodes if num_nodes > 0 else 0

# Step 3: Create a DataFrame for a better overview
degree_distribution = pd.DataFrame({
    'Node ID': list(G.nodes),
    'Num Neighbors': degrees
})

# Step 4: Summary statistics of the degree distribution
degree_stats = degree_distribution['Num Neighbors'].describe()

# Step 5: Output the results
print(f"Total number of nodes: {num_nodes}")
print(f"Total number of edges: {num_edges}")
print(f"Minimum number of neighbors: {min_neighbors}")
print(f"Maximum number of neighbors: {max_neighbors}")
print(f"Average number of neighbors: {avg_neighbors:.2f}")
print("\nDegree Distribution Summary:")
print(degree_stats)


import numpy as np

def random_walk(start, length):
    walk = [str(start)]  # Starting node

    for i in range(length):
        # Get the neighbors of the current node
        neighbors = [node for node in G.neighbors(start)]

        # Check if the node has neighbors
        if len(neighbors) == 0:
            next_node = start  # If no neighbors, stay in the same node
        else:
            next_node = np.random.choice(neighbors, 1)[0]  # Choose a neighbor at random

        walk.append(str(next_node))
        start = next_node  # Move to the next node

    return walk

nx.is_connected(G)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import GradientBoostingRegressor
# from gensim.models.word2vec import Word2Vec

def fit_and_evaluate(model, X_train, y_train, X_test, y_test, filename=None, verbose=True, tolerance=0.2):
    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Check for zero or negative values in y_test and y_pred
    epsilon = 1e-10  # Small positive value to replace zeros
    mean_y_test = np.mean(y_test)  # Mean value of y_test

    # Replace zero or negative values in y_test
    if (y_test <= 0).any():
        print("y_test contains zero or negative values. Replacing with mean.")
        y_test = np.where(y_test <= 0, mean_y_test, y_test)  # Replace with mean_y_test

    # Replace zero or negative values in y_pred
    if (y_pred <= 0).any():
        print("y_pred contains zero or negative values. Replacing with mean.")
        y_pred = np.where(y_pred <= 0, mean_y_test, y_pred)  # Replace with mean_y_test

    # Metrics
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mse_log = mean_squared_error(np.log10(y_test), np.log10(y_pred))

    # Range accuracy
    correct_count = np.sum(np.abs(y_test - y_pred) <= tolerance * y_test)
    accuracy = correct_count / len(y_test)

    # Scatterplot
    if verbose:
        plt.scatter(y_test, y_pred)
        plt.xlabel('Actual Prices')
        plt.ylabel('Predicted Prices')
        plt.title('Actual vs Predicted Prices')
        if filename:  # Save only if filename is provided
            plt.savefig(filename, format='png')
        plt.close()

    if verbose:
        print(f"R2 Score: {r2}, MAPE: {mape}, RMSE: {rmse}, Accuracy: {accuracy}")

    return r2, mape, accuracy, rmse, mse_log

def create_word2vec_model(walks, vector_size):
    model = Word2Vec(walks,
                     hs=1,   # Hierarchical softmax
                     sg=1,   # Skip-gram
                     vector_size=vector_size,
                     window=5,
                     workers=4,
                     seed=1)
    return model


def get_embeddings(model, G):
    return np.array([model.wv[str(i)] for i in G.nodes()])

def word2vec_model(df, G, vector_size):
    walks = generate_random_walks(G)
    wv_model = create_word2vec_model(walks, vector_size=vector_size)#, p=.5, q=4)
    embeddings = get_embeddings(wv_model, G)
    # print(node2vec_embeddings.shape)
    embeddings_df = pd.DataFrame(embeddings, columns=[f'node2vec_embedding_{i}' for i in range(embeddings.shape[1])])
    # df_with_embeddings = pd.concat([df_filtered.reset_index(drop=True), embeddings_df], axis=1)
    df_with_embeddings = pd.concat([df, embeddings_df], axis=1)

    # Prepare for Regression
    X = df_with_embeddings.drop(['price', 'id'], axis=1)
    y = df_with_embeddings['price']
    return X, y

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import GradientBoostingRegressor
import matplotlib.pyplot as plt

def grid_search_embedding_size(df, G, embedding_sizes, score_name='rmse', random_state=42):
    # Initialize best score and parameters
    best_score = np.inf if score_name == 'rmse' else -np.inf
    best_params = None
    best_X = None
    best_y = None

    # To store scores for each embedding size
    results = []

    for vector_size in embedding_sizes:
        print(f"Evaluating embedding size: {vector_size}")

        X, y = word2vec_model(df, G, vector_size)
        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=random_state)

        # Cross-validation
        kf = KFold(n_splits=5, shuffle=True, random_state=random_state)
        scores = []

        for train_index, val_index in kf.split(X_train):
            X_train_kf, X_val_kf = X_train.iloc[train_index], X_train.iloc[val_index]
            y_train_kf, y_val_kf = y_train.iloc[train_index], y_train.iloc[val_index]

            model = GradientBoostingRegressor(loss='huber', n_estimators=100, max_depth=10, random_state=random_state)
            if score_name == 'rmse':
                _, _, _, score, _ = fit_and_evaluate(model, X_train_kf, y_train_kf, X_val_kf, y_val_kf, verbose=False)
            else:
                score, _, _, _, _ = fit_and_evaluate(model, X_train_kf, y_train_kf, X_val_kf, y_val_kf, verbose=False)
            scores.append(score)

        mean_score = np.mean(scores)
        print(f"Mean {score_name} for embedding size {vector_size}: {mean_score}")

        # Store results
        results.append((vector_size, mean_score))

        # Update best score and parameters
        if score_name == 'rmse':
            condition = mean_score < best_score
        else:
            condition = mean_score > best_score
        if condition == True:
            best_score = mean_score
            best_params = vector_size
            best_X = X
            best_y = y

    print(f"Best embedding size: {best_params} with {score_name}: {best_score}")

    # Create DataFrame from results
    results_df = pd.DataFrame(results, columns=['Embedding Size', score_name])

    return best_params, best_X, best_y, results_df  # Return best parameters, X, y, and results DataFrame

def plot_results(results_df, best_params, best_score, score_name):
    # Plotting using matplotlib for visualization
    plt.figure(figsize=(12, 8))
    plt.plot(results_df['Embedding Size'], results_df[score_name], marker='o', color='blue', label=score_name.upper())
    plt.scatter(best_params, best_score, color='red', label=f'Best Embedding Size ({best_params})')
    plt.xlabel('Embedding Size')
    plt.ylabel(score_name.upper())
    plt.xticks(results_df['Embedding Size'])
    plt.title(f'Embedding Size vs {score_name.upper()}')
    plt.grid(True)
    plt.savefig('results/CA/embedding_size_plot.png')  # Save the plot as an image
    plt.show()

df.head(1)


import pickle

data_set_name = 'CA'
# Collect results into a dictionary
grid_search_results = {
    'data_set_name' : data_set_name,
    'best_embedding_size': best_embedding_size,
    'X': X,
    'y': y,
    'results_df': results_df,
    'score_name': score_name,
    'embedding_sizes': embedding_sizes
}

# Generate a filename based on the parameters
filename = f"var/grid_search_results_{data_set_name}_{score_name}_embedding_{'_'.join(map(str, embedding_sizes))}.pkl"

# Save the results to a pickle file
with open(filename, 'wb') as f:
    pickle.dump(grid_search_results, f)

print(f"Grid search results saved to {filename}")


# Save results to Excel
results_df.to_excel('results/CA/embedding_size_results.xlsx', index=False)

# Plotting
plot_results(results_df, best_embedding_size, results_df.loc[results_df['Embedding Size'] == best_embedding_size].iloc[0][score_name], score_name)

"""Re-creation of node representation, based on the best embedding size"""

# best_embedding_size = 15
X, y = word2vec_model(df, G, best_embedding_size)

X_train_w_embeddings, X_test_w_embeddings, y_train, y_test = train_test_split(X, y,\
                                                test_size=0.1, random_state=42)
X_train_wo_embeddings = X_train_w_embeddings.iloc[:, :-best_embedding_size]
X_test_wo_embeddings = X_test_w_embeddings.iloc[:, :-best_embedding_size]

import pickle
# List of variables to save
data_to_save = {
    'X': X,
    'y': y,
    'X_train_wo_embeddings': X_train_wo_embeddings,
    'X_train_w_embeddings': X_train_w_embeddings,
    'X_test_wo_embeddings': X_test_wo_embeddings,
    'X_test_w_embeddings': X_test_w_embeddings,
    'y_train': y_train,
    'y_test': y_test,
    'best_embedding_size': best_embedding_size,
}

# Save the variables to a pickle file
with open('var/data_deepwalk-CA.pkl', 'wb') as f:
    pickle.dump(data_to_save, f)

print("Variables have been saved to var folder")


"""## Without Word2Vec"""

model = GradientBoostingRegressor( loss = 'huber' , n_estimators=100 ,  max_depth=10 , random_state=5 )
r2, mape , accuracy, rmse, mse_log = fit_and_evaluate(model, X_train_wo_embeddings, \
                            y_train, X_test_wo_embeddings, y_test,'results/CA/prediction_wo-deepwalk.png')

print(f"R2 Score: {r2:.2f}")
print(f"MAPE: {mape:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MSE (log): {mse_log:.2f}")
print(f"acc: {accuracy:.2f}")
final_score = (r2 + (1 - mape) + accuracy) / 3
print(f'Final score: {final_score:.2f}')

# Fit the model on the entire training set
model_final = GradientBoostingRegressor(loss='huber', n_estimators=100, max_depth=10, random_state=5)
r2_with_deepwalk, mape_with_deepwalk, accuracy_with_deepwalk, rmse_with_deepwalk, mse_log_with_deepwalk = fit_and_evaluate(model_final,\
            X_train_w_embeddings, y_train, X_test_w_embeddings, y_test, 'results/CA/prediction_w-deepwalk.png', verbose=True)

print(f"R2 Score: {r2_with_deepwalk:.2f}")
print(f"MAPE: {mape_with_deepwalk:.2f}")
print(f"RMSE: {rmse_with_deepwalk:.2f}")
print(f"MSE (log): {mse_log_with_deepwalk:.2f}")
print(f"Accuracy: {accuracy_with_deepwalk:.2f}")
final_score_deepwalk = (r2_with_deepwalk + (1 - mape_with_deepwalk) + accuracy_with_deepwalk) / 3
print(f'Final score: {final_score_deepwalk:.2f}')

import numpy as np
from scipy import stats

# این مقادیر از مدل ها به دست آمده اند.
n_samples = X_train_w_embeddings.shape[0]  #تعداد نمونه های داده ها
rmse_without_deepwalk = rmse  # MSE مدل بدون DeepWalk
# rmse_with_deepwalk = 0.8195510061449558 # MSE مدل با DeepWalk
num_features_without = X_train_wo_embeddings.shape[1] #تعداد ویژگی های مدل بدون DeepWalk
num_features_with = X_train_w_embeddings.shape[1]    #تعداد ویژگی های مدل با DeepWalk
num_added_params = num_features_with - num_features_without #تعداد پارامترهای اضافه شده (10)


# محاسبه درجات آزادی (Degrees of Freedom)
df_without = n_samples - num_features_without -1 # برای رگرسیون خطی
df_with = n_samples - num_features_with -1      # برای رگرسیون خطی

#محاسبه آماره F
F = ((rmse_without_deepwalk - rmse_with_deepwalk) / num_added_params) / (rmse_with_deepwalk / df_with)

# محاسبه‌ی p-value
p_value = 1 - stats.f.cdf(F, num_added_params, df_with)

print(f"F-statistic: {F:.2f}")
print(f"P-value: {p_value:.3f}")

# تفسیر نتایج
alpha = 0.05
if p_value < alpha:
    print("تفاوت بین دو مدل معنی‌دار است (p-value < 0.05).")
else:
    print("تفاوت بین دو مدل معنی‌دار نیست (p-value >= 0.05).")

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression

# Gradient Boosting Regressor
model_gb = GradientBoostingRegressor(loss='huber', n_estimators=100, max_depth=10, random_state=5)
r2_gb, mape_gb, accuracy_gb, rmse_gb, mse_log_gb = fit_and_evaluate(model_gb, X_train_wo_embeddings,\
        y_train, X_test_wo_embeddings, y_test, 'results/CA/wo-deepwalk.png')

r2_gb_deepwalk, mape_gb_deepwalk, accuracy_gb_deepwalk, rmse_gb_deepwalk, mse_log_gb_deepwalk = fit_and_evaluate(model_gb,
        X_train_w_embeddings, y_train, X_test_w_embeddings, y_test,
        'results/CA/deepwalk_pred_final.png', verbose=True)

# Linear Regression Model
model_lr = LinearRegression()

# Fit and evaluate the model without embeddings
r2_lr, mape_lr, accuracy_lr, rmse_lr, mse_log_lr = fit_and_evaluate(model_lr,
        X_train_wo_embeddings, y_train,
        X_test_wo_embeddings, y_test,
        'results/CA/wo-lr.png')

# Fit and evaluate the model with embeddings
r2_lr_deepwalk, mape_lr_deepwalk, accuracy_lr_deepwalk, rmse_lr_deepwalk, mse_log_lr_deepwalk = fit_and_evaluate(model_lr,
        X_train_w_embeddings, y_train,
        X_test_w_embeddings, y_test,
        'results/CA/deepwalk_lr_pred_final.png', verbose=True)

# Random Forest Regressor
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=5)
r2_rf, mape_rf, accuracy_rf, rmse_rf, mse_log_rf = fit_and_evaluate(model_rf,\
        X_train_wo_embeddings, y_train,  \
        X_test_wo_embeddings, y_test, 'results/CA/wo-rf.png')

r2_rf_deepwalk, mape_rf_deepwalk, accuracy_rf_deepwalk, rmse_rf_deepwalk, mse_log_rf_deepwalk = fit_and_evaluate(model_rf,
        X_train_w_embeddings, y_train, X_test_w_embeddings, y_test,
        'results/CA/deepwalk_rf_pred_final.png', verbose=True)

# Create a DataFrame to store results
results_df = pd.DataFrame({
    'Model': ['Gradient Boosting',
              'Gradient Boosting (DeepWalk)',
              'Linear Regression',
              'Linear Regression (DeepWalk)',
              'Random Forest',
              'Random Forest (DeepWalk)'],
    'R2 Score': [r2_gb,
                 r2_gb_deepwalk,
                 r2_lr,
                 r2_lr_deepwalk,
                 r2_rf,
                 r2_rf_deepwalk],
    'MAPE': [mape_gb,
             mape_gb_deepwalk,
             mape_lr,
             mape_lr_deepwalk,
             mape_rf,
             mape_rf_deepwalk],
    'Accuracy': [accuracy_gb,
                 accuracy_gb_deepwalk,
                 accuracy_lr,
                 accuracy_lr_deepwalk,
                 accuracy_rf,
                 accuracy_rf_deepwalk],
    'RMSE': [rmse_gb,
            rmse_gb_deepwalk,
            rmse_lr,
            rmse_lr_deepwalk,
            rmse_rf,
            rmse_rf_deepwalk],
    'MSE_log': [mse_log_gb,
            mse_log_gb_deepwalk,
            mse_log_lr,
            mse_log_lr_deepwalk,
            mse_log_rf,
            mse_log_rf_deepwalk]
})

# Save the DataFrame to an Excel file
results_df.to_excel('results/CA/model_results.xlsx', index=False)

print("Results have been saved to model_results.xlsx")
results_df

import pandas as pd

# Load the DataFrame from Excel file
file_path = 'results/CA/model_results.xlsx'
results_df = pd.read_excel(file_path)

# Format specific columns
df_style = results_df.style.format({
    'R2 Score': '{:.2f}',
    'MAPE': '{:.2f}',
    'Accuracy': '{:.2f}',
    'RMSE': '{:.2f}',
    'MSE_log': '{:.3f}'
})

# Display the formatted DataFrame
df_style

# Create a new column to indicate if the model is DeepWalk or not
results_df['Type'] = results_df['Model'].apply(lambda x: 'With DeepWalk' if 'DeepWalk' in x else 'Without DeepWalk')

# Group the data by Type and Model to get the mean R2 Score and RMSE
summary_df = results_df.groupby(['Type', 'Model']).agg({'R2 Score': 'mean', 'RMSE': 'mean'}).reset_index()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import FuncFormatter

# Load the DataFrame from the Excel file
file_path = 'results/CA/model_results.xlsx'
results_df = pd.read_excel(file_path)

# Create a new column to indicate if the model is DeepWalk or not
results_df['Type'] = results_df['Model'].apply(lambda x: 'With DeepWalk' if 'DeepWalk' in x else 'Without DeepWalk')

# Group the data by Model to get the mean R2 Score and RMSE
summary_df = results_df.groupby(['Type', 'Model']).agg({'R2 Score': 'mean', 'RMSE': 'mean'}).reset_index()

# Prepare data for plotting
no_deepwalk = summary_df[summary_df['Type'] == 'Without DeepWalk']
deepwalk = summary_df[summary_df['Type'] == 'With DeepWalk']

# Calculate means for R2 Score and RMSE
r2_mean = summary_df['R2 Score'].mean()
rmse_mean = summary_df['RMSE'].mean()

# Set the figure size for the plots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Number of models
num_models = len(no_deepwalk)

# Set the bar width and x locations for the bars
bar_width = 0.35
x = np.arange(num_models)

# Create bar plot for R2 Score comparison
axs[0].bar(x - bar_width/2, no_deepwalk['R2 Score'], width=bar_width, label='Without DeepWalk')
axs[0].bar(x + bar_width/2, deepwalk['R2 Score'], width=bar_width, label='With DeepWalk')
axs[0].set_title('R2 Score Comparison')
axs[0].set_xlabel('Model')
axs[0].set_ylabel('R2 Score')
axs[0].set_xticks(x)
axs[0].set_xticklabels(no_deepwalk['Model'], rotation=45)
axs[0].legend(title='Type')

# Set the y-axis limits for R2 Score plot
axs[0].set_ylim(r2_mean - 0.2, 0.9)  # Adjust the lower limit based on your data range

# تابع قالب‌بندی برای محور y
def millions(x, pos):
    return '%1.1f' % (x * 1e-5)

formatter = FuncFormatter(millions)

# Create bar plot for RMSE comparison
axs[1].bar(x - bar_width/2, no_deepwalk['RMSE'], width=bar_width, label='Without DeepWalk')
axs[1].bar(x + bar_width/2, deepwalk['RMSE'], width=bar_width, label='With DeepWalk')
axs[1].set_title('RMSE Comparison')
axs[1].set_xlabel('Model')
axs[1].set_ylabel('Root Mean Squared Error (RMSE)')
axs[1].set_xticks(x)
axs[1].set_xticklabels(no_deepwalk['Model'], rotation=45)
axs[1].legend(title='Type')

# اعمال قالب‌بندی محور y axs[
axs[1].yaxis.set_major_formatter(formatter)

# Set the y-axis limits for RMSE plot
axs[1].set_ylim(rmse_mean * 0.7, rmse_mean * 1.4)  # Adjust limits to give a better visualization

# Adjust layout to prevent overlap
plt.tight_layout()

# Save the plot as a PNG file
plt.savefig('results/CA/model_comparison_side_by_side.png')

# Show the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import FuncFormatter

# Load the DataFrame from the Excel file
file_path = 'results/CA/model_results.xlsx'
results_df = pd.read_excel(file_path)

# Create a new column to indicate if the model is DeepWalk or not
results_df['Type'] = results_df['Model'].apply(lambda x: 'With DeepWalk' if 'DeepWalk' in x else 'Without DeepWalk')

# Group the data by Model to get the mean MAPE and MSE_log
summary_df = results_df.groupby(['Type', 'Model']).agg({'MAPE': 'mean', 'MSE_log': 'mean'}).reset_index()

# Prepare data for plotting
no_deepwalk = summary_df[summary_df['Type'] == 'Without DeepWalk']
deepwalk = summary_df[summary_df['Type'] == 'With DeepWalk']

# Calculate means for MAPE and MSE_log
mape_mean = summary_df['MAPE'].mean()
mse_log_mean = summary_df['MSE_log'].mean()

# Set the figure size for the plots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Number of models
num_models = len(no_deepwalk)

# Set the bar width and x locations for the bars
bar_width = 0.35
x = np.arange(num_models)

# Create bar plot for MAPE comparison
axs[0].bar(x - bar_width/2, no_deepwalk['MAPE'], width=bar_width, label='Without DeepWalk')
axs[0].bar(x + bar_width/2, deepwalk['MAPE'], width=bar_width, label='With DeepWalk')
axs[0].set_title('MAPE Comparison')
axs[0].set_xlabel('Model')
axs[0].set_ylabel('Mean Absolute Percentage Error (MAPE)')
axs[0].set_xticks(x)
axs[0].set_xticklabels(no_deepwalk['Model'], rotation=45)
axs[0].legend(title='Type')

# Set the y-axis limits for MAPE plot
axs[0].set_ylim(mape_mean - 0.1, mape_mean + 0.1)  # Adjust the limits based on your data range

# Create bar plot for MSE_log comparison
axs[1].bar(x - bar_width/2, no_deepwalk['MSE_log'], width=bar_width, label='Without DeepWalk')
axs[1].bar(x + bar_width/2, deepwalk['MSE_log'], width=bar_width, label='With DeepWalk')
axs[1].set_title('MSE_log Comparison')
axs[1].set_xlabel('Model')
axs[1].set_ylabel('Mean Squared Error (log scale)')
axs[1].set_xticks(x)
axs[1].set_xticklabels(no_deepwalk['Model'], rotation=45)
axs[1].legend(title='Type')

# Set the y-axis limits for MSE_log plot
axs[1].set_ylim(mse_log_mean - 0.01, mse_log_mean + 0.015)  # Adjust limits to give a better visualization

# Adjust layout to prevent overlap
plt.tight_layout()

# Save the plot as a PNG file
plt.savefig('results/CA/mape_mse_log_comparison_side_by_side.png')

# Show the plot
plt.show()

import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Calculate basic statistics
num_nodes = G.number_of_nodes()
num_edges = G.number_of_edges()
degrees = [len(list(G.neighbors(node))) for node in G.nodes]  # List of number of neighbors for each node

# Step 2: Compute minimum, maximum, and average number of neighbors
min_neighbors = min(degrees)
max_neighbors = max(degrees)
avg_neighbors = sum(degrees) / num_nodes if num_nodes > 0 else 0

# Step 3: Create a DataFrame for a better overview
degree_distribution = pd.DataFrame({
    'Node ID': list(G.nodes),
    'Num Neighbors': degrees
})

# Step 4: Summary statistics of the degree distribution
degree_stats = degree_distribution['Num Neighbors'].describe()

# Step 5: Output the results
print(f"Total number of nodes: {num_nodes}")
print(f"Total number of edges: {num_edges}")
print(f"Minimum number of neighbors: {min_neighbors}")
print(f"Maximum number of neighbors: {max_neighbors}")
print(f"Average number of neighbors: {avg_neighbors:.2f}")
print("\nDegree Distribution Summary:")
print(degree_stats)

import pandas as pd

# Load results from the Excel file
results_df = pd.read_excel('results/CA/model_results.xlsx')

# Generate LaTeX longtable format
latex_table = results_df.to_latex(index=False, escape=False, longtable=True, caption='Results of Regression Models', label='tab:regression_results')

# Specify the filename for the LaTeX table
latex_filename = 'results/CA/results_table.tex'

# Save the LaTeX table to a file
with open(latex_filename, 'w') as f:
    f.write(latex_table)

print(f"LaTeX table has been saved to {latex_filename}")

